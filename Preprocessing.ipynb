{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "import json\n",
    "from pprint import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=43\n",
    "random.seed=SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training Dataset\"\"\"\n",
    "with open('train-v2.0.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hyperparameters\"\"\"\n",
    "word_embedding_dim=300\n",
    "char_embedding_dim=300\n",
    "q_words=50\n",
    "c_words=399\n",
    "vocab=[0]\n",
    "max_word_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert GloVe file to a dictionary\"\"\"\n",
    "def loadGloveModel(gloveFile):\n",
    "    print \"Loading Glove Model\"\n",
    "    f = open(gloveFile,'r')\n",
    "    model = dict()\n",
    "    embedding = []\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        model[word]=np.array([float(val) for val in splitLine[1:]])\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 2196016  words loaded!\n",
      "Loading Glove Model\n",
      "Done. 94  words loaded!\n",
      "CPU times: user 5min, sys: 8.31 s, total: 5min 8s\n",
      "Wall time: 5min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Load word as well as character embedding\"\"\"\n",
    "model = loadGloveModel(\"GloVe/glove.840B.300d.txt\")\n",
    "model_char = loadGloveModel(\"glove.840B.300d-char.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=model_char.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reduce a sentence to its word embedding, character embedding, a boolean vector which \n",
    "tells if any word is in GloVe dictionary or not and an integer array of start positions of every word\"\"\"\n",
    "def preprocess(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    temp = pos_tag(word_tokenize(sentence))\n",
    "    y = []\n",
    "    word_emb = []\n",
    "    char_emb = []\n",
    "    word_in_glove = []\n",
    "    count=0\n",
    "    word_zeros = np.zeros((300),dtype=float)\n",
    "    for i,j in temp:\n",
    "        y.append(count)\n",
    "        if i==u'``' or i==u\"''\":\n",
    "            x='\"'\n",
    "            count+=1\n",
    "        else:\n",
    "            if j[0].lower() in ['a','n','v']:\n",
    "                #lemmatization\n",
    "                temp_i = wnl.lemmatize(i,j[0].lower())\n",
    "                # unicode normalization\n",
    "                x=unicodedata.normalize('NFKD', temp_i).encode('ascii','ignore')\n",
    "            else:\n",
    "                temp_i = wnl.lemmatize(i)\n",
    "                x=unicodedata.normalize('NFKD', temp_i).encode('ascii','ignore')\n",
    "            count+=len(i)\n",
    "        while count<len(sentence) and sentence[count]==' ':\n",
    "            count+=1\n",
    "        try:\n",
    "            word_emb.append(model[x])\n",
    "            word_in_glove.append(1)\n",
    "        except:\n",
    "            word_emb.append(word_zeros)\n",
    "            word_in_glove.append(0)\n",
    "        temp_char = []\n",
    "        for k in range(len(x)):\n",
    "            try:\n",
    "                temp_char.append(vocab.index(x[k]))\n",
    "            except:\n",
    "                pass\n",
    "        temp_char+=[len(vocab) for _ in range(max_word_len-len(temp_char))]\n",
    "        char_emb.append(temp_char)\n",
    "    char_emb=np.array(char_emb)\n",
    "    word_emb=np.array(word_emb)\n",
    "    return word_emb, char_emb, word_in_glove, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Traverses over whole dataset and preprocesses every context and question, and builds an array of the dataset in\n",
    "vectorized form\"\"\"\n",
    "def create_dataset(data,test=True):\n",
    "    data_map = []\n",
    "    errors = 0\n",
    "    t_count=0\n",
    "    file_count=0\n",
    "    random.shuffle(data['data'])\n",
    "    for i in data['data']:\n",
    "        t_count+=1\n",
    "        print str(t_count)+str(\".\"),\n",
    "        print i['title'],\n",
    "        print \"Errors=\"+str(errors)\n",
    "        for j in i['paragraphs']:\n",
    "            c_word,c_char,c_bool,c_idx = preprocess(j['context'])\n",
    "            if len(c_word)>c_words:\n",
    "                continue\n",
    "#             for x in range(len(idx)-1):\n",
    "#                 print idx[x], j['context'][idx[x]:idx[x+1]]\n",
    "#             print context\n",
    "#             print idx\n",
    "            for k in j['qas']:\n",
    "                temp = {}\n",
    "                q_word,q_char,q_bool,q_idx = preprocess(k['question'])\n",
    "                if len(q_word)>q_words:\n",
    "                    continue\n",
    "                temp['title']=i['title']\n",
    "                temp['context']=j['context']\n",
    "                temp['contextword']=c_word\n",
    "                temp['contextchar']=c_char\n",
    "                temp['questionword']=q_word\n",
    "                temp['questionchar']=q_char\n",
    "                temp['contextbool']=c_bool\n",
    "                temp['questionbool']=q_bool\n",
    "                temp['is_impossible']=k['is_impossible']\n",
    "                temp['context_indices']=c_idx\n",
    "                try:\n",
    "                    if k['is_impossible']:\n",
    "                        if len(k['plausible_answers'])==0:\n",
    "                            continue\n",
    "                        #if impossible, select 1st plausible answer\n",
    "                        ans_temp = k['plausible_answers'][0]\n",
    "                        ans = pos_tag(word_tokenize(ans_temp['text'].lower()))\n",
    "                        temp['plausible_start']=c_idx.index(ans_temp['answer_start'])\n",
    "                        temp['plausible_end']=c_idx.index(ans_temp['answer_start'])+len(ans)\n",
    "                        # set answer as []\n",
    "                        temp['answer_start']=0\n",
    "                        temp['answer_end']=0\n",
    "                        temp['answer']=ans_temp['text']\n",
    "                        data_map.append(temp)\n",
    "\n",
    "                    else:\n",
    "                        if len(k['answers'])==0:\n",
    "                            continue\n",
    "                        # if possible, select 1st answer\n",
    "                        ans_temp = k['answers'][0]\n",
    "                        ans = pos_tag(word_tokenize(ans_temp['text'].lower()))\n",
    "                        temp['answer_start']=c_idx.index(ans_temp['answer_start'])\n",
    "                        temp['answer_end']=c_idx.index(ans_temp['answer_start'])+len(ans)\n",
    "                        # set plausible answer as []\n",
    "                        temp['plausible_start']=0\n",
    "                        temp['plausible_end']=0\n",
    "                        temp['answer']=ans_temp['text']\n",
    "                        data_map.append(temp)\n",
    "                except:\n",
    "                    errors+=1\n",
    "                    \n",
    "    if test==False:\n",
    "        # Pickle train_dataset\n",
    "        with open(\"train_data_unans.pkl\",'w') as f:\n",
    "            pickle.dump(data_map,f)\n",
    "            print(\"Dumped\")\n",
    "    else:\n",
    "        # Pickle test dataset\n",
    "        with open('test_data_unans.pkl','w') as f:\n",
    "            pickle.dump(data_map,f)      \n",
    "    print \"Errors=\"+str(errors)\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Preprocess every data item for training dataset\"\"\"\n",
    "dm = create_dataset(data,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev-v2.0.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Preprocess every data item for testing dataset\"\"\"\n",
    "data = create_dataset(data,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
