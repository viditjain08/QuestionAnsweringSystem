{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import collections\n",
    "import time\n",
    "from math import log10\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 43\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert GloVe file to a dictionary\"\"\"\n",
    "def loadGloveModel(gloveFile):\n",
    "    print \"Loading Glove Model\"\n",
    "    f = open(gloveFile,'r')\n",
    "    model = dict()\n",
    "    embedding = []\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        model[word]=np.array([float(val) for val in splitLine[1:]])\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 2196016  words loaded!\n",
      "Loading Glove Model\n",
      "Done. 94  words loaded!\n",
      "CPU times: user 3min 39s, sys: 6.61 s, total: 3min 45s\n",
      "Wall time: 3min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Load word as well as character embedding\"\"\"\n",
    "model = loadGloveModel(\"GloVe/glove.840B.300d.txt\")\n",
    "model_char = loadGloveModel(\"glove.840B.300d-char.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reduce a sentence to its word embedding, character embedding, a boolean vector which \n",
    "tells if any word is in GloVe dictionary or not and an integer array of start positions of every word\"\"\"\n",
    "def preprocess(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    temp = pos_tag(word_tokenize(sentence))\n",
    "    y = []\n",
    "    word_emb = []\n",
    "    char_emb = []\n",
    "    word_in_glove = []\n",
    "    count=0\n",
    "    word_zeros = np.zeros((300),dtype=float)\n",
    "    for i,j in temp:\n",
    "        y.append(count)\n",
    "        if i==u'``' or i==u\"''\":\n",
    "            x='\"'\n",
    "            count+=1\n",
    "        else:\n",
    "            if j[0].lower() in ['a','n','v']:\n",
    "                temp_i = wnl.lemmatize(i,j[0].lower())\n",
    "                x=unicodedata.normalize('NFKD', temp_i).encode('ascii','ignore')\n",
    "            else:\n",
    "                temp_i = wnl.lemmatize(i)\n",
    "                x=unicodedata.normalize('NFKD', temp_i).encode('ascii','ignore')\n",
    "            count+=len(i)\n",
    "        while count<len(sentence) and sentence[count]==' ':\n",
    "            count+=1\n",
    "        try:\n",
    "            word_emb.append(model[x])\n",
    "            word_in_glove.append(1)\n",
    "        except:\n",
    "            word_emb.append(word_zeros)\n",
    "            word_in_glove.append(0)\n",
    "        temp_char = []\n",
    "        for k in range(len(x)):\n",
    "            try:\n",
    "                temp_char.append(vocab.index(x[k]))\n",
    "            except:\n",
    "                pass\n",
    "        temp_char+=[len(vocab) for _ in range(max_word_len-len(temp_char))]\n",
    "        char_emb.append(temp_char)\n",
    "    char_emb=np.array(char_emb)\n",
    "    word_emb=np.array(word_emb)\n",
    "    return word_emb, char_emb, word_in_glove, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 300)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load vocabulary from character embedding GloVe dictionary\"\"\"\n",
    "vocab_emb = []\n",
    "for k in model_char.keys():\n",
    "    vocab_emb.append(model_char[k])\n",
    "vocab_emb.append(np.zeros((300)))\n",
    "vocab_emb = np.array(vocab_emb)\n",
    "print(vocab_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=list(model_char.keys())+[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hyperparameters\"\"\"\n",
    "max_word_len=40\n",
    "num_gpus=4\n",
    "word_embedding_dim=300\n",
    "char_embedding_dim=300\n",
    "q_words=50\n",
    "c_words=399\n",
    "c_words_u=400\n",
    "embedding_dim = word_embedding_dim+char_embedding_dim\n",
    "batch_size=8\n",
    "batch_size_o=(num_gpus)*batch_size\n",
    "model_encoder_layers=3\n",
    "no_of_chars = len(vocab)\n",
    "hidden_layer_size=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initializers and regularizers\"\"\"\n",
    "\n",
    "initializer = lambda: tf.contrib.layers.variance_scaling_initializer(factor=1.0,\n",
    "                                                             mode='FAN_AVG',\n",
    "                                                             uniform=True,\n",
    "                                                             dtype=tf.float32)\n",
    "initializer_relu = lambda: tf.contrib.layers.variance_scaling_initializer(factor=2.0,\n",
    "                                                             mode='FAN_IN',\n",
    "                                                             uniform=False,\n",
    "                                                             dtype=tf.float32)\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale = 3e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stochastic Depth\"\"\"\n",
    "def layer_dropout(inputs, residual, dropout):\n",
    "    pred = tf.random_uniform([]) < dropout\n",
    "    pred = tf.cast(pred,tf.float32)\n",
    "    ifdrop = tf.nn.dropout(inputs, 1.0 - dropout) + residual\n",
    "    return pred*residual+(1-pred)*ifdrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Applies a 2-dimensional convolution over a 3-d or 3-d matrix with bias and activation if specified\"\"\"\n",
    "def conv(inputs, output_size, bias = None, activation = None, kernel_size = 1, name = \"conv\", reuse=None):\n",
    "    with tf.variable_scope(name,reuse=reuse):\n",
    "        def_shape=4\n",
    "        if len(inputs.shape)==3:\n",
    "            inputs = tf.expand_dims(inputs,axis=1)\n",
    "            def_shape=3\n",
    "        shapes = inputs.shape.as_list()\n",
    "        filter_shape = [1,kernel_size,shapes[-1],output_size]\n",
    "        bias_shape = [1,1,1,output_size]\n",
    "        strides = [1,1,1,1]\n",
    "        kernel_ = tf.get_variable(\"kernel_\",\n",
    "                        filter_shape,\n",
    "                        dtype = tf.float32,\n",
    "                        regularizer=regularizer,\n",
    "                        initializer = initializer_relu() if activation is not None else initializer())\n",
    "        outputs = tf.nn.conv2d(inputs, kernel_, strides, \"VALID\")\n",
    "        if bias:\n",
    "            outputs += tf.get_variable(\"bias_\",\n",
    "                        bias_shape,\n",
    "                        regularizer= tf.contrib.layers.l2_regularizer(scale = 3e-7),\n",
    "                        initializer = tf.zeros_initializer())\n",
    "        if def_shape==3:\n",
    "            outputs = tf.squeeze(outputs,axis=1)\n",
    "        if activation is not None:\n",
    "            return activation(outputs)\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Depthwise Separable Convolutions\"\"\"\n",
    "def depthconv(x, kernel_size, output_filters, scope_name,reuse=None):\n",
    "    with tf.variable_scope(scope_name,reuse=reuse):\n",
    "        shapes = x.shape.as_list()\n",
    "        depthwise_filter = tf.get_variable(\"depthwise_filter\",\n",
    "                                        (kernel_size[0], kernel_size[1], shapes[-1], 1),\n",
    "                                        dtype = tf.float32,\n",
    "                                        regularizer=regularizer,\n",
    "                                        initializer = initializer_relu())\n",
    "        pointwise_filter = tf.get_variable(\"pointwise_filter\",\n",
    "                                        (1,1,shapes[-1],output_filters),\n",
    "                                        dtype = tf.float32,\n",
    "                                        regularizer=regularizer,\n",
    "                                        initializer = initializer_relu())\n",
    "        outputs = tf.nn.separable_conv2d(x,\n",
    "                                        depthwise_filter,\n",
    "                                        pointwise_filter,\n",
    "                                        strides = (1,1,1,1),\n",
    "                                        padding = \"SAME\")\n",
    "        b = tf.get_variable(\"bias\",\n",
    "                outputs.shape[-1],\n",
    "                regularizer=tf.contrib.layers.l2_regularizer(scale = 3e-7),\n",
    "                initializer = tf.zeros_initializer())\n",
    "        outputs += b\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Applies a single head attention layer over V using softmax(QK/d**0.5)V\"\"\"\n",
    "def dot_product_attention(q,k,v,dropout,scope=\"dot_product_attention\"):\n",
    "    \"\"\"\n",
    "    q: a Tensor with shape [batch, heads, length_q, depth_k]\n",
    "    k: a Tensor with shape [batch, heads, length_kv, depth_k]\n",
    "    v: a Tensor with shape [batch, heads, length_kv, depth_v]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        # [batch, num_heads, query_length, memory_length]\n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "        logits = logits/(k.shape.as_list()[-1]**0.5)\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "        # dropping out the attention links for each of the heads\n",
    "        weights = tf.nn.dropout(weights, 1.0 - dropout)\n",
    "        return tf.matmul(weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Applies dot attention in parallel with multiplicity equal to the number of heads specified\"\"\"\n",
    "def multihead_attention(queries, units, num_heads, dropout, memory = None, scope = \"Multi_Head_Attention\",reuse=None):\n",
    "    with tf.variable_scope(scope,reuse=reuse):\n",
    "        # Self attention\n",
    "        if memory is None:\n",
    "            memory = queries\n",
    "        memory = conv(memory, 2 * units, name = \"memory_projection\",reuse=reuse)\n",
    "        query = conv(queries, units, name = \"query_projection\",reuse=reuse)\n",
    "        qshapes = query.shape.as_list()\n",
    "        Q = tf.reshape(query, [qshapes[0],qshapes[1],num_heads,-1])\n",
    "        Q = tf.transpose(Q,[0,2,1,3])\n",
    "\n",
    "        mshapes = memory.shape.as_list()\n",
    "        M = tf.reshape(memory, [qshapes[0],qshapes[1],num_heads*2,-1])\n",
    "        M = tf.transpose(M,[0,2,1,3])\n",
    "        K, V = tf.split(M,2,axis=1)\n",
    "\n",
    "        x = dot_product_attention(Q,K,V,dropout)\n",
    "        \n",
    "        shapes = x.shape.as_list()\n",
    "        return tf.reshape(tf.transpose(x,[0,2,1,3]),[shapes[0],shapes[2],-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Applies num_conv_layers convolutions after layer normalization in num_blocks, then a multihead attention\n",
    "    and finally, a feed forward layer\"\"\"\n",
    "def encoderblock(x, kernel_size, output_filters, num_conv_layers, scope_name, drop, num_blocks=1, reuse=None):\n",
    "    with tf.variable_scope(scope_name, reuse=reuse):\n",
    "        x = tf.expand_dims(x,axis=1)\n",
    "        x = conv(x,output_filters,name=\"conv0\",reuse=reuse)\n",
    "        for _ in range(num_blocks):\n",
    "            with tf.variable_scope(\"Block\"+str(_)):\n",
    "                for i in range(num_conv_layers):\n",
    "                    if (i) % 2 == 0:\n",
    "                        x = tf.nn.dropout(x, 1-drop)\n",
    "                    if len(x.shape.as_list())==3:\n",
    "                        x=tf.expand_dims(x,axis=1)\n",
    "                    y = tf.contrib.layers.layer_norm(x,scope=\"layer_norm1_%d\"%i,reuse=reuse)\n",
    "                    y = depthconv(y, kernel_size, output_filters, 'dconv'+str(i),reuse=reuse)\n",
    "                    x = layer_dropout(y,x,drop*float(_*(num_conv_layers+2)+i+1)/float((num_conv_layers + 2) * num_blocks))\n",
    "                x_res1 = tf.squeeze(x, axis=1)\n",
    "                x = tf.contrib.layers.layer_norm(x_res1,scope=\"layer_norm2\",reuse=reuse)\n",
    "                x = tf.nn.dropout(x,1-drop)\n",
    "                x = multihead_attention(x,output_filters,6,reuse=reuse,dropout=drop)\n",
    "                x_res2 = layer_dropout(x,x_res1,drop*float(_*(num_conv_layers+2)+num_conv_layers+1)/float((num_conv_layers + 2) * num_blocks))\n",
    "\n",
    "                x = tf.contrib.layers.layer_norm(x_res2,scope=\"layer_norm3\",reuse=reuse)\n",
    "                x = tf.nn.dropout(x,1-drop)\n",
    "                x = conv(x,output_filters,True,activation=tf.nn.relu,name=\"FFN1\",reuse=reuse)\n",
    "                x = conv(x,output_filters,True,activation=None,name=\"FFN2\",reuse=reuse)\n",
    "                x = layer_dropout(x,x_res2,drop*float(_*(num_conv_layers+2)+num_conv_layers+2)/float((num_conv_layers + 2) * num_blocks))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q=tf.placeholder(tf.float32,[batch_size,q_words,embedding_dim])\n",
    "# print encoderblock(q,(7,1),128,4,\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"An n-layer Highway network with each layer consisting of 2 convolutions\"\"\"\n",
    "def highway(x, drop, size = None, activation = None, num_layers = 2, scope = \"highway\",reuse=None):\n",
    "    with tf.variable_scope(scope,reuse=reuse):\n",
    "        if size is None:\n",
    "            size = x.shape.as_list()[-1]\n",
    "        else:\n",
    "            x = conv(x, size, name = \"input_projection\",reuse=reuse)\n",
    "        for i in range(num_layers):\n",
    "            T = conv(x, size, bias = True, activation = tf.sigmoid,\n",
    "                     name = \"gate_%d\"%i,reuse=reuse)\n",
    "            H = conv(x, size, bias = True, activation = activation,\n",
    "                     name = \"activation_%d\"%i,reuse=reuse)\n",
    "            H = tf.nn.dropout(H, 1-drop)\n",
    "            x = H * T + x * (1.0 - T)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list ranges\n",
    "        over the devices. The inner list ranges over the different variables.\n",
    "    Returns:\n",
    "            List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "            across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads=[]\n",
    "        for g, _ in grad_and_vars:\n",
    "            if g is not None:\n",
    "                grads.append(g)\n",
    "        if grads==[]:\n",
    "            print grad_and_vars[0][1]\n",
    "            continue\n",
    "        grad = tf.reduce_mean(tf.stack(grads), 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Starting Learning Rate\"\"\"\n",
    "starter_learning_rate = tf.placeholder(tf.float32,shape=[])\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "var_ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           4000, 1, staircase=True)\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ema_getter(getter, name, *args, **kwargs):\n",
    "#     var = getter(name, *args, **kwargs)\n",
    "#     ema_var = ema.average(var)\n",
    "#     return ema_var if ema_var else var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Placeholders\"\"\"\n",
    "tower_grads = []\n",
    "losses = []\n",
    "drop = tf.placeholder_with_default(0.0, shape=())\n",
    "question_bool = tf.placeholder(tf.float32,[batch_size,q_words],name=\"question_bool\")\n",
    "question_word = tf.placeholder(tf.float32,[batch_size,q_words,word_embedding_dim],name=\"question_word\")\n",
    "question_char = tf.placeholder(tf.int32,[batch_size,q_words,None],name=\"question_char\")\n",
    "context_bool = tf.placeholder(tf.float32,[batch_size,c_words],name=\"context_bool\")\n",
    "context_word = tf.placeholder(tf.float32,[batch_size,c_words,word_embedding_dim],name=\"context_word\")\n",
    "context_char = tf.placeholder(tf.int32,[batch_size,c_words,None],name=\"context_char\")\n",
    "start_ans = tf.placeholder(tf.int32,[batch_size],name=\"start_ans\")\n",
    "end_ans = tf.placeholder(tf.int32,[batch_size],name=\"end_ans\")\n",
    "impossible = tf.placeholder(tf.int32,[batch_size],name=\"impossible\")\n",
    "start_plausible = tf.placeholder(tf.int32,[batch_size],name=\"start_plausible\")\n",
    "end_plausible = tf.placeholder(tf.int32,[batch_size],name=\"end_plausible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Model/Model/add_1:0\", shape=(8, 50, 300), dtype=float32)\n",
      "Tensor(\"Model/Model/Input_Embedding_Layer/concat_2:0\", shape=(8, 400, 396), dtype=float32)\n",
      "Tensor(\"Model/Model/Context-query_attention/Squeeze:0\", shape=(8, 400, 50), dtype=float32)\n",
      "Tensor(\"Model/Model/Model_encoder_layer/encoder_layer_1/Block7/add_7:0\", shape=(8, 400, 96), dtype=float32)\n",
      "Tensor(\"Model/Model/Model_encoder_layer/encoder_layer_2/Block7/add_7:0\", shape=(8, 400, 96), dtype=float32)\n",
      "Tensor(\"Model/Model/Output_Layer/Softmax:0\", shape=(8, 400), dtype=float32)\n",
      "Tensor(\"Model/Model/Output_Layer/Softmax_2:0\", shape=(8, 400), dtype=float32)\n",
      "Tensor(\"Model/Model/Output_Layer/one_hot:0\", shape=(8, 400), dtype=float32)\n",
      "Tensor(\"Model/Model/Output_Layer/transpose:0\", shape=(8, 1, 400), dtype=float32)\n",
      "Tensor(\"Model/Model/Output_Layer/Squeeze_4:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"Model/Model/Output_Layer/sub_5:0\", shape=(8,), dtype=float32)\n",
      "Tensor(\"Model/Model/Output_Layer/add_13:0\", shape=(8,), dtype=float32)\n",
      "Exponential Moving Average\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Model\"):\n",
    "    final_logits1_ans = []\n",
    "    final_logits2_ans = []\n",
    "    final_logits1_unans = []\n",
    "    final_logits2_unans = []\n",
    "    answer_v = []\n",
    "    \n",
    "    \"\"\"Random initialization of some required variables\"\"\"\n",
    "\n",
    "    universal = tf.get_variable(\"universal\",(word_embedding_dim+hidden_layer_size),dtype = tf.float32,\n",
    "                                            initializer = initializer())\n",
    "    unk = tf.get_variable(\"unk\",(word_embedding_dim),dtype = tf.float32,\n",
    "                                            initializer = initializer())\n",
    "    char_emb = tf.get_variable(\"char_emb\",dtype = tf.float32,\n",
    "                                            initializer =tf.constant(vocab_emb, dtype=tf.float32))\n",
    "    zero_word = tf.constant(np.zeros((300)),dtype=tf.float32)\n",
    "    flag=0\n",
    "    with tf.variable_scope(tf.get_variable_scope()) as outer_scope:\n",
    "        universal_tiled = tf.expand_dims(tf.expand_dims(universal,axis=0),axis=0)\n",
    "        universal_tiled = tf.tile(universal_tiled,[batch_size,1,1])\n",
    "        unk_tiled = tf.expand_dims(tf.expand_dims(unk,axis=0),axis=0)\n",
    "        zero_tiled = tf.expand_dims(tf.expand_dims(zero_word,axis=0),axis=0)\n",
    "\n",
    "        question_bool1 = tf.expand_dims(question_bool,axis=-1)\n",
    "        context_bool1 = tf.expand_dims(context_bool,axis=-1)\n",
    "#         print question_bool1\n",
    "        question_word = (question_bool1*(2-question_bool1)*question_word)+(0.5*(1-question_bool1)*(2-question_bool1)*unk_tiled)+(0.5*question_bool1*(question_bool1-1)*zero_tiled)\n",
    "        print question_word\n",
    "        question_word = tf.nn.dropout(question_word,1-drop)\n",
    "\n",
    "        context_word = (context_bool1*(2-context_bool1)*context_word)+(0.5*(1-context_bool1)*(2-context_bool1)*unk_tiled)+(0.5*context_bool1*(context_bool1-1)*zero_tiled)\n",
    "        context_word = tf.nn.dropout(context_word,1-drop)\n",
    "\n",
    "\n",
    "#         print char_emb\n",
    "        question_char_new = tf.nn.embedding_lookup(char_emb,question_char)\n",
    "        question_char_new = tf.nn.dropout(question_char_new,1-(drop*0.5))\n",
    "\n",
    "#         print question_char_new\n",
    "        context_char_new = tf.nn.embedding_lookup(char_emb,context_char)\n",
    "        context_char_new = tf.nn.dropout(context_char_new,1-(drop*0.5))\n",
    "\n",
    "#         print context_char_new\n",
    "        with tf.variable_scope(\"Input_Embedding_Layer\"):\n",
    "            \"\"\"Convolution over char embedding, max pooling and highway network\"\"\"\n",
    "            question_char1 = conv(question_char_new,hidden_layer_size,kernel_size=5,name=\"q_char_conv0\",activation=tf.nn.relu)\n",
    "            question_char1 = tf.reduce_max(question_char1,axis=2)\n",
    "            question_emb = tf.concat([question_word,question_char1],axis=-1)\n",
    "            question_emb = highway(question_emb, drop, scope=\"highway\")\n",
    "#                     print question_emb\n",
    "\n",
    "            \"\"\"Convolution over char embedding, max pooling, universal node concatenation and highway network\"\"\"\n",
    "            context_char1 = conv(context_char_new,hidden_layer_size,kernel_size=5,name=\"c_char_conv0\",activation=tf.nn.relu)\n",
    "            context_char1 = tf.reduce_max(context_char1,axis=2)\n",
    "            context_emb = tf.concat([context_word,context_char1],axis=-1)\n",
    "            context_emb = tf.concat([universal_tiled,context_emb],axis=1)\n",
    "            print(context_emb)\n",
    "            context_emb = highway(context_emb, drop, scope=\"highway\",reuse=True)\n",
    "#             print context_emb\n",
    "\n",
    "\n",
    "        with tf.variable_scope(\"Embedding_encoding_layer\"):\n",
    "            question = encoderblock(question_emb, (1,7), hidden_layer_size,scope_name=\"encoder_block\",num_conv_layers=4,drop=drop)\n",
    "#             print question\n",
    "\n",
    "            context = encoderblock(context_emb, (1,7), hidden_layer_size,scope_name=\"encoder_block\",num_conv_layers=4,reuse=True,drop=drop)\n",
    "#             print context\n",
    "\n",
    "        with tf.variable_scope(\"Context-query_attention\"):\n",
    "            \"\"\"Similarity matrix computation\"\"\"\n",
    "            question = tf.nn.dropout(question,1-drop)\n",
    "            q = tf.tile(tf.expand_dims(question,axis=1),[1,c_words_u,1,1])\n",
    "            context = tf.nn.dropout(context,1-drop)\n",
    "            c = tf.tile(tf.expand_dims(context,axis=2),[1,1,q_words,1])\n",
    "            s = conv(tf.concat([q,c,tf.multiply(q,c)],axis=-1),1,name=\"similarity_matrix\")\n",
    "            s = tf.squeeze(s,axis=-1)\n",
    "            print s\n",
    "            s_ = tf.nn.softmax(s,axis=-1)\n",
    "            a = tf.matmul(s_,question)\n",
    "#             print a\n",
    "            b = tf.matmul(s_, tf.matmul(tf.transpose(tf.nn.softmax(s,axis=1),[0,2,1]),context))\n",
    "#             print b\n",
    "\n",
    "        with tf.variable_scope(\"Model_encoder_layer\"):\n",
    "            \"\"\"3 encoder blocks with shared parameters\"\"\"\n",
    "            enc_input = tf.concat([context,a,context*a,context*b],axis=-1)\n",
    "#             print enc_input\n",
    "            enc_input = conv(enc_input,hidden_layer_size,name=\"conv0\")\n",
    "            enc_input = encoderblock(enc_input, kernel_size=(1,7), output_filters=hidden_layer_size,scope_name=\"encoder_layer\", num_conv_layers=2, num_blocks=8,drop=drop)\n",
    "#             print enc_input\n",
    "            output_list = [enc_input]\n",
    "            for i in range(model_encoder_layers-1):\n",
    "                temp = encoderblock(output_list[i], kernel_size=(1,7), output_filters=hidden_layer_size,scope_name=\"encoder_layer\", num_conv_layers=2, num_blocks=8,drop=drop,reuse=True)\n",
    "                print temp\n",
    "                output_list.append(temp)\n",
    "\n",
    "        with tf.variable_scope(\"Output_Layer\"):\n",
    "            \"\"\"Softmax followed by loss function calculation\"\"\"\n",
    "            start_logits_ans = tf.squeeze(conv(tf.concat([output_list[0], output_list[1]],axis = -1),1, bias = False, name = \"start_pointer_ans\"),-1)\n",
    "            end_logits_ans = tf.squeeze(conv(tf.concat([output_list[0], output_list[2]],axis = -1),1, bias = False, name = \"end_pointer_ans\"), -1)\n",
    "\n",
    "            start_logits_unans = tf.squeeze(conv(tf.concat([output_list[0], output_list[1]],axis = -1),1, bias = False, name = \"start_pointer_unans\"),-1)\n",
    "            end_logits_unans = tf.squeeze(conv(tf.concat([output_list[0], output_list[2]],axis = -1),1, bias = False, name = \"end_pointer_unans\"), -1)\n",
    "\n",
    "            logits1_ans = tf.nn.softmax(start_logits_ans)\n",
    "            logits2_ans = tf.nn.softmax(end_logits_ans)\n",
    "            print logits1_ans\n",
    "            final_logits1_ans.append(logits1_ans)\n",
    "            final_logits2_ans.append(logits2_ans)\n",
    "\n",
    "\n",
    "            logits1_unans = tf.nn.softmax(start_logits_unans)\n",
    "            logits2_unans = tf.nn.softmax(end_logits_unans)\n",
    "            print logits1_unans\n",
    "            final_logits1_unans.append(logits1_unans)\n",
    "            final_logits2_unans.append(logits2_unans)                    \n",
    "\n",
    "\n",
    "            imp = tf.cast(impossible,tf.float32)\n",
    "            start = tf.one_hot(start_ans,c_words_u)\n",
    "            end = tf.one_hot(end_ans,c_words_u)\n",
    "            start_plaus = tf.one_hot(start_plausible,c_words_u)\n",
    "            end_plaus = tf.one_hot(end_plausible,c_words_u)\n",
    "            print start\n",
    "            loss1 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=start_logits_ans,labels=start)\n",
    "            loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=end_logits_ans,labels=end)\n",
    "            loss3 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=start_logits_unans,labels=start_plaus)\n",
    "            loss4 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=end_logits_unans,labels=end_plaus)\n",
    "\n",
    "            loss5 = -imp*(tf.math.log(logits1_ans[:,0]+1e-06)+tf.math.log(logits2_ans[:,0]+1e-06))\n",
    "            loss6 = -(1-imp)*(tf.math.log((1-logits1_ans[:,0])+1e-06)+tf.math.log((1-logits2_ans[:,0])+1e-06))\n",
    "            answer_verifier = tf.concat([output_list[0], output_list[1], output_list[2]],axis=-1)\n",
    "            verifier_start = tf.expand_dims(logits1_ans,axis=-1)\n",
    "            verifier_end = tf.expand_dims(logits2_ans,axis=-1)\n",
    "            av = tf.concat([verifier_start*answer_verifier,verifier_end*answer_verifier],axis=-1)\n",
    "            av = conv(av,1,bias=False,name=\"av1\",activation=tf.nn.tanh)\n",
    "            av = tf.transpose(av,[0,2,1])\n",
    "            print av\n",
    "            av = conv(av,1,bias=False,activation=tf.nn.sigmoid,name=\"av2\")\n",
    "            av = tf.squeeze(av)\n",
    "            print av\n",
    "            answer_v.append(av)\n",
    "            loss7 = (-imp*tf.math.log(1-av+1e-06)-(1-imp)*tf.math.log(av+1e-06))\n",
    "            print loss7\n",
    "            loss = (1-imp)*(loss1+loss2)+imp*(loss3+loss4)+loss5+loss6+loss7\n",
    "            print loss\n",
    "            if flag==0:\n",
    "                \"\"\"Apply exponential moving average for better training of variables\"\"\"\n",
    "                print(\"Exponential Moving Average\")\n",
    "                ema_op = var_ema.apply(tf.trainable_variables())\n",
    "                flag=1\n",
    "            with tf.control_dependencies([ema_op]):\n",
    "                loss = tf.identity(loss)\n",
    "\n",
    "            grads = optimizer.compute_gradients(loss)\n",
    "            tower_grads.append(grads)\n",
    "\n",
    "\n",
    "\n",
    "            losses.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Apply regularization to loss, and assign average values from ema to all the variables\"\"\"\n",
    "\n",
    "gradients = average_gradients(tower_grads)\n",
    "avg_loss = tf.reduce_mean(tf.stack(losses))\n",
    "apply_gradient_op = optimizer.apply_gradients(gradients, global_step)\n",
    "logits1_f = tf.concat(final_logits1_ans, axis=0)\n",
    "logits2_f = tf.concat(final_logits2_ans, axis=0)\n",
    "answer_v_f = tf.concat(answer_v, axis=0)\n",
    "# params = tf.trainable_variables()\n",
    "l2_loss = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(scale = 3e-7),tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "avg_loss+=l2_loss\n",
    "\n",
    "assign_vars = []\n",
    "for var in tf.global_variables():\n",
    "    v = var_ema.average(var)\n",
    "    if v:\n",
    "        assign_vars.append(tf.assign(var,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Random variables initialization\"\"\"\n",
    "init_op = tf.global_variables_initializer()\n",
    "config=tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_writer = tf.summary.FileWriter( './logs/1/train ', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_model_unans_final/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, \"./saved_model_unans_final/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"F1 score calculation from predicted answer and actual answer in list format\"\"\"\n",
    "def compute_f1(ans_predicted, ans_actual):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    for gold_toks, pred_toks in zip(ans_actual, ans_predicted):\n",
    "        common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "        num_same = sum(common.values())\n",
    "        tp+=num_same\n",
    "        fp+=(len(pred_toks)-num_same)\n",
    "        tn+=(len(gold_toks)-num_same)\n",
    "    return tp,fp,tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data_unans_demo.pkl\") as f:\n",
    "    test_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_of_batches = int(len(train_data)/batch_size_o)\n",
    "epoch = 50\n",
    "prev_f1=0\n",
    "cur_f1=0\n",
    "max_f1=0\n",
    "# print no_of_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Converts a mini batch of training dataset, applies required padding and returns a feed dict\"\"\"\n",
    "def train(x):\n",
    "    feed_dict={}\n",
    "    q_w = []\n",
    "    q_c = []\n",
    "    c_w = []\n",
    "    c_c = []\n",
    "    a_s = []\n",
    "    a_e = []\n",
    "    q_b = []\n",
    "    c_b = []\n",
    "    s_p = []\n",
    "    e_p = []\n",
    "    imp = []\n",
    "    for i in x:\n",
    "        zero_w = np.zeros((q_words-len(i['questionword']),word_embedding_dim))\n",
    "        q_w.append(np.concatenate((i['questionword'],zero_w),axis=0))\n",
    "        zero_w = np.zeros((c_words-len(i['contextword']),word_embedding_dim))\n",
    "        c_w.append(np.concatenate((i['contextword'],zero_w),axis=0)) \n",
    "        q_b.append(np.pad(i['questionbool'],[0,q_words-len(i['questionword'])],'constant', constant_values=(2)))\n",
    "        c_b.append(np.pad(i['contextbool'],[0,c_words-len(i['contextword'])],'constant', constant_values=(2)))\n",
    "        zero_c = (no_of_chars-1)*np.ones((q_words-len(i['questionchar']),max_word_len))\n",
    "        q_c.append(np.concatenate((i['questionchar'],zero_c),axis=0))\n",
    "        zero_c = (no_of_chars-1)*np.ones((c_words-len(i['contextchar']),max_word_len))\n",
    "        c_c.append(np.concatenate((i['contextchar'],zero_c),axis=0))\n",
    "        a_s.append(i['answer_start'])\n",
    "        a_e.append(i['answer_end'])\n",
    "        s_p.append(i['plausible_start'])\n",
    "        e_p.append(i['plausible_end'])\n",
    "        imp.append(i['is_impossible'])\n",
    "    feed_dict = {\n",
    "        question_word:np.array(q_w),\n",
    "        context_word:np.array(c_w),\n",
    "        question_char:np.array(q_c),\n",
    "        context_char:np.array(c_c),\n",
    "        start_ans:np.array(a_s),\n",
    "        end_ans:np.array(a_e),\n",
    "        question_bool:np.array(q_b),\n",
    "        context_bool:np.array(c_b),\n",
    "        impossible:np.array(imp),\n",
    "        start_plausible:np.array(s_p),\n",
    "        end_plausible:np.array(e_p)\n",
    "    }\n",
    "\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate results\"\"\"\n",
    "def test(x_test,max_f1,testing=True,threshold=0.7):\n",
    "        tp=0\n",
    "        fp=0\n",
    "        tn=0\n",
    "        print len(x_test)\n",
    "        no_of_test_batches=len(x_test)/(batch_size)\n",
    "        print no_of_test_batches\n",
    "        t_ptr=0\n",
    "        for tbatch in range(min(no_of_test_batches,100)):\n",
    "            test_feed = train(x_test[t_ptr:t_ptr+batch_size])\n",
    "            np_ans_st = sess.run(logits1_f,test_feed)\n",
    "            np_ans_e = sess.run(logits2_f,test_feed)\n",
    "            np_av = sess.run(answer_v_f,test_feed)\n",
    "            ans_st = np.argmax(np_ans_st[:,1:],axis=-1)\n",
    "            ans_e = np.argmax(np_ans_e[:,1:],axis=-1)\n",
    "            ans_pred=[]\n",
    "            ans_ac=[]\n",
    "            for i in range(batch_size):\n",
    "                if np_av[i]<=threshold:\n",
    "                    ans_pred.append([''])\n",
    "                else:\n",
    "                    temp = x_test[t_ptr+i]['context']\n",
    "                    temp2 = x_test[t_ptr+i]['context_indices']\n",
    "                    try:\n",
    "                        ans_pred.append(normalize_answer(temp[temp2[ans_st[i]]:temp2[ans_e[i]]]).split())\n",
    "                    except:\n",
    "                        try:\n",
    "                            ans_pred.append(normalize_answer(temp[temp2[ans_st[i]]:]).split())\n",
    "                        except:\n",
    "                            ans_pred.append([-1])\n",
    "                if x_test[t_ptr+i]['is_impossible']==1:\n",
    "                    ans_ac.append([''])\n",
    "                else:\n",
    "                    temp = x_test[t_ptr+i]['context']\n",
    "                    temp2 = x_test[t_ptr+i]['context_indices']\n",
    "                    try:\n",
    "                        ans_ac.append(normalize_answer(temp[temp2[test_feed[start_ans][i]-1]:temp2[test_feed[end_ans][i]-1]]).split())\n",
    "                    except:\n",
    "                        try:\n",
    "                            ans_ac.append(normalize_answer(temp[temp2[test_feed[start_ans][i]-1]:]).split())\n",
    "                        except:\n",
    "                            ans_ac.append([-1])\n",
    "            x,y,z=compute_f1(ans_pred, ans_ac)\n",
    "            tp+=x\n",
    "            fp+=y\n",
    "            tn+=z\n",
    "            t_ptr+=batch_size\n",
    "\n",
    "            if tbatch%10==0:\n",
    "                print(\"Testing %d\" % tbatch),\n",
    "                if testing:\n",
    "                    print \"On testing data\"\n",
    "                else:\n",
    "                    print \"On training data\"\n",
    "                print \"Start: \",np.argmax(np_ans_st[:,1:],axis=-1)[0],\n",
    "                print \"End: \",np.argmax(np_ans_e[:,1:],axis=-1)[0],\n",
    "                print \"Start Actual: \",test_feed[start_ans][0]-1,\n",
    "                print \"End Actual: \",test_feed[end_ans][0]-1\n",
    "                print \"Answer verifier: \",np_av[0]\n",
    "                print \"Answer predicted \",ans_pred[0]\n",
    "                print \"Answer Actual: \",ans_ac[0]\n",
    "\n",
    "        print tp,fp,tn\n",
    "        try:\n",
    "            precision = 1.0 * tp / (tp+fp)\n",
    "            recall = 1.0 * tp / (tp+tn)\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "            print(\"F1 Score:\")+str(f1)\n",
    "        except:\n",
    "            print(\"F1 Score:N/A\")\n",
    "            if testing:\n",
    "                pass\n",
    "#                 save_path = saver.save(sess, \"./saved_model_unans/model.ckpt\")\n",
    "#                 print(\"Model saved in path: %s\" % save_path)\n",
    "            return max_f1\n",
    "        if (f1>max_f1 or sess.run(global_step) <15000) and testing:\n",
    "#             save_path = saver.save(sess, \"./saved_model_unans_final/model.ckpt\")\n",
    "#             print(\"Model saved in path: %s\" % save_path)\n",
    "            max_f1 = max(f1,max_f1)\n",
    "        else:\n",
    "            print \"Not Saved\"\n",
    "        return max(max_f1,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "100\n",
      "Testing 0 On training data\n",
      "Start:  0 End:  1 Start Actual:  0 End Actual:  1\n",
      "Answer verifier:  0.28542066\n",
      "Answer predicted  ['']\n",
      "Answer Actual:  [u'fresno']\n",
      "Testing 10 On training data\n",
      "Start:  24 End:  30 Start Actual:  24 End Actual:  30\n",
      "Answer verifier:  0.91820663\n",
      "Answer predicted  [u'kings', u'canyon', u'avenue', u'and', u'clovis', u'avenue']\n",
      "Answer Actual:  [u'kings', u'canyon', u'avenue', u'and', u'clovis', u'avenue']\n",
      "Testing 20 On training data\n",
      "Start:  66 End:  67 Start Actual:  89 End Actual:  91\n",
      "Answer verifier:  0.97746086\n",
      "Answer predicted  [u'235']\n",
      "Answer Actual:  [u'300', u'acres']\n",
      "Testing 30 On training data\n",
      "Start:  0 End:  1 Start Actual:  0 End Actual:  1\n",
      "Answer verifier:  0.5586515\n",
      "Answer predicted  ['']\n",
      "Answer Actual:  [u'fresno']\n",
      "Testing 40 On training data\n",
      "Start:  87 End:  89 Start Actual:  87 End Actual:  89\n",
      "Answer verifier:  0.9957301\n",
      "Answer predicted  [u'sweyn', u'forkbeard']\n",
      "Answer Actual:  [u'sweyn', u'forkbeard']\n",
      "Testing 50 On training data\n",
      "Start:  5 End:  10 Start Actual:  -1 End Actual:  -1\n",
      "Answer verifier:  0.9988232\n",
      "Answer predicted  [u'king', u'of', u'canary', u'islands']\n",
      "Answer Actual:  ['']\n",
      "Testing 60 On training data\n",
      "Start:  69 End:  70 Start Actual:  -1 End Actual:  -1\n",
      "Answer verifier:  0.5649754\n",
      "Answer predicted  ['']\n",
      "Answer Actual:  ['']\n",
      "Testing 70 On training data\n",
      "Start:  58 End:  60 Start Actual:  -1 End Actual:  -1\n",
      "Answer verifier:  0.9168908\n",
      "Answer predicted  [u'daniel', u'andrews']\n",
      "Answer Actual:  ['']\n",
      "Testing 80 On training data\n",
      "Start:  50 End:  53 Start Actual:  -1 End Actual:  -1\n",
      "Answer verifier:  0.98072547\n",
      "Answer predicted  [u'grand', u'annual', u'steeplechase']\n",
      "Answer Actual:  ['']\n",
      "Testing 90 On training data\n",
      "Start:  54 End:  56 Start Actual:  53 End Actual:  56\n",
      "Answer verifier:  0.9965849\n",
      "Answer predicted  [u'scalar', u'quantities']\n",
      "Answer Actual:  [u'denoted', u'scalar', u'quantities']\n",
      "747 758 446\n",
      "F1 Score:0.553743513714\n",
      "Not Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5537435137138621"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_data,0,False,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(global_step):\n",
    "    if global_step>1000:\n",
    "        return 0.0002\n",
    "    return (0.001/3)*log10(global_step+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_func(x):\n",
    "    feed_dict={}\n",
    "    q_w = []\n",
    "    q_c = []\n",
    "    c_w = []\n",
    "    c_c = []\n",
    "    a_s = []\n",
    "    a_e = []\n",
    "    q_b = []\n",
    "    c_b = []\n",
    "    for i in x:\n",
    "        zero_w = np.zeros((q_words-len(i['questionword']),word_embedding_dim))\n",
    "        q_w.append(np.concatenate((i['questionword'],zero_w),axis=0))\n",
    "        zero_w = np.zeros((c_words-len(i['contextword']),word_embedding_dim))\n",
    "        c_w.append(np.concatenate((i['contextword'],zero_w),axis=0)) \n",
    "        q_b.append(np.pad(i['questionbool'],[0,q_words-len(i['questionword'])],'constant', constant_values=(2)))\n",
    "        c_b.append(np.pad(i['contextbool'],[0,c_words-len(i['contextword'])],'constant', constant_values=(2)))\n",
    "        zero_c = (no_of_chars-1)*np.ones((q_words-len(i['questionchar']),max_word_len))\n",
    "        q_c.append(np.concatenate((i['questionchar'],zero_c),axis=0))\n",
    "        zero_c = (no_of_chars-1)*np.ones((c_words-len(i['contextchar']),max_word_len))\n",
    "        c_c.append(np.concatenate((i['contextchar'],zero_c),axis=0))\n",
    "    feed_dict = {\n",
    "        question_word:np.array(q_w),\n",
    "        context_word:np.array(c_w),\n",
    "        question_char:np.array(q_c),\n",
    "        context_char:np.array(c_c),\n",
    "        question_bool:np.array(q_b),\n",
    "        context_bool:np.array(c_b)\n",
    "    }\n",
    "\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Returns the answer to the question given context\"\"\"\n",
    "kabidef get_answer(context,question):\n",
    "    c_word,c_char,c_bool,c=preprocess(context)\n",
    "    q_word,q_char,q_bool,q=preprocess(question)\n",
    "    data = {'contextword':c_word,'contextchar':c_char,'contextbool':c_bool,'questionword':q_word,'questionchar':q_char,'questionbool':q_bool}\n",
    "    example_feed = [data for _ in range(batch_size)]\n",
    "    temp1 = np.argmax(sess.run(logits1_ans,testing_func(example_feed))[0])\n",
    "    temp2 = np.argmax(sess.run(logits2_ans,testing_func(example_feed))[0])\n",
    "    if sess.run(answer_v,testing_func(example_feed))[0][0]<0.6:\n",
    "        return \"Impossible question\"\n",
    "    try:\n",
    "        return context[c[temp1-1]:c[temp2-1]]\n",
    "    except:\n",
    "        return context[c[temp1-1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=u\"\"\"Michael John Clarke (born 2 April 1981) is a former Australian international cricketer. He led Australia to their 5th Cricket World Cup triumph, when his team were victorious in the final of the ICC Cricket World Cup 2015 at the MCG.[2]. He is the first cap for Australia in Twenty20 Internationals. His ODI shirt number of 23 was passed on to him by Shane Warne after his international retirement. Nicknamed \"Pup\",[3] he is a right-handed middle-order batsman, an occasional left-arm orthodox spin bowler and also a slip catcher. He represented New South Wales at a domestic level. In January 2011, Clarke stood down as captain of the Australian Twenty20 cricket team to concentrate on his Test and ODI performance. After announcing he would retire from One Day cricket after the end of the 2015 Cricket World Cup, Clarke starred in the final against New Zealand top scoring with a score of 74 off 72 balls, as Australia won their fifth World Cup title. He was bowled when nine runs were required to win and received a standing ovation from the 93,013 strong MCG crowd after his dismissal. On 8 August 2015, Clarke announced that he would retire from all forms of cricket after the final Test of the 2015 Ashes series following a difficult series in terms of both his and the team's performance. Australia suffered a crushing defeat of an innings and 78 runs thus losing the Ashes. This was Clarke's fourth successive Ashes loss in England overall and his second as captain.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=u\"\"\"What is the nickname of Michael Clarke?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impossible question\n"
     ]
    }
   ],
   "source": [
    "print(get_answer(context,question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
