{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import unicodedata\n",
    "from pprint import pprint\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import time\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert GloVe file to a dictionary\"\"\"\n",
    "def loadGloveModel(gloveFile):\n",
    "    print \"Loading Glove Model\"\n",
    "    f = open(gloveFile,'r')\n",
    "    model = dict()\n",
    "    embedding = []\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        model[word]=np.array([float(val) for val in splitLine[1:]])\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Load word as well as character embedding\"\"\"\n",
    "model = loadGloveModel(\"GloVe/glove.840B.300d.txt\")\n",
    "model_char = loadGloveModel(\"glove.840B.300d-char.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reduce a sentence to its word embedding, character embedding, a boolean vector which \n",
    "tells if any word is in GloVe dictionary or not and an integer array of start positions of every word\"\"\"\n",
    "def preprocess(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    temp = pos_tag(word_tokenize(sentence))\n",
    "    y = []\n",
    "    word_emb = []\n",
    "    char_emb = []\n",
    "    word_in_glove = []\n",
    "    count=0\n",
    "    word_zeros = np.zeros((300),dtype=float)\n",
    "    for i,j in temp:\n",
    "        y.append(count)\n",
    "        if i==u'``' or i==u\"''\":\n",
    "            x='\"'\n",
    "            count+=1\n",
    "        else:\n",
    "            if j[0].lower() in ['a','n','v']:\n",
    "                #lemmatization\n",
    "                temp_i = wnl.lemmatize(i,j[0].lower())\n",
    "                # unicode normalization\n",
    "                x=unicodedata.normalize('NFKD', temp_i).encode('ascii','ignore')\n",
    "            else:\n",
    "                temp_i = wnl.lemmatize(i)\n",
    "                x=unicodedata.normalize('NFKD', temp_i).encode('ascii','ignore')\n",
    "            count+=len(i)\n",
    "        while count<len(sentence) and sentence[count]==' ':\n",
    "            count+=1\n",
    "        try:\n",
    "            word_emb.append(model[x])\n",
    "            word_in_glove.append(1)\n",
    "        except:\n",
    "            word_emb.append(word_zeros)\n",
    "            word_in_glove.append(0)\n",
    "        temp_char = []\n",
    "        for k in range(len(x)):\n",
    "            try:\n",
    "                temp_char.append(vocab.index(x[k]))\n",
    "            except:\n",
    "                pass\n",
    "        temp_char+=[len(vocab) for _ in range(max_word_len-len(temp_char))]\n",
    "        char_emb.append(temp_char)\n",
    "    char_emb=np.array(char_emb)\n",
    "    word_emb=np.array(word_emb)\n",
    "    return word_emb, char_emb, word_in_glove, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load vocabulary from character embedding GloVe dictionary\"\"\"\n",
    "vocab_emb = []\n",
    "for k in model_char.keys():\n",
    "    vocab_emb.append(model_char[k])\n",
    "vocab_emb.append(np.zeros((300)))\n",
    "vocab_emb = np.array(vocab_emb)\n",
    "print(vocab_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=list(model_char.keys())+[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hyperparameters\"\"\"\n",
    "max_word_len=40\n",
    "num_gpus=3\n",
    "word_embedding_dim=300\n",
    "char_embedding_dim=300\n",
    "q_words=50\n",
    "c_words=400\n",
    "embedding_dim = word_embedding_dim+char_embedding_dim\n",
    "batch_size=8\n",
    "batch_size_o=(num_gpus)*batch_size\n",
    "model_encoder_layers=3\n",
    "no_of_chars = len(vocab)\n",
    "hidden_layer_size=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initializers and regularizers\"\"\"\n",
    "initializer = lambda: tf.contrib.layers.variance_scaling_initializer(factor=1.0,\n",
    "                                                             mode='FAN_AVG',\n",
    "                                                             uniform=True,\n",
    "                                                             dtype=tf.float32)\n",
    "initializer_relu = lambda: tf.contrib.layers.variance_scaling_initializer(factor=2.0,\n",
    "                                                             mode='FAN_IN',\n",
    "                                                             uniform=False,\n",
    "                                                             dtype=tf.float32)\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale = 3e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stochastic Depth\"\"\"\n",
    "def layer_dropout(inputs, residual, dropout):\n",
    "    pred = tf.random_uniform([]) < dropout\n",
    "    pred = tf.cast(pred,tf.float32)\n",
    "    ifdrop = tf.nn.dropout(inputs, 1.0 - dropout) + residual\n",
    "    return pred*residual+(1-pred)*ifdrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Applies a 2-dimensional convolution over a 3-d or 3-d matrix with bias and activation if specified\"\"\"\n",
    "def conv(inputs, output_size, bias = None, activation = None, kernel_size = 1, name = \"conv\", reuse=None):\n",
    "    with tf.variable_scope(name,reuse=reuse):\n",
    "        def_shape=4\n",
    "        if len(inputs.shape)==3:\n",
    "            inputs = tf.expand_dims(inputs,axis=1)\n",
    "            def_shape=3\n",
    "        shapes = inputs.shape.as_list()\n",
    "        filter_shape = [1,kernel_size,shapes[-1],output_size]\n",
    "        bias_shape = [1,1,1,output_size]\n",
    "        strides = [1,1,1,1]\n",
    "        kernel_ = tf.get_variable(\"kernel_\",\n",
    "                        filter_shape,\n",
    "                        dtype = tf.float32,\n",
    "                        regularizer=regularizer,\n",
    "                        initializer = initializer_relu() if activation is not None else initializer())\n",
    "        outputs = tf.nn.conv2d(inputs, kernel_, strides, \"VALID\")\n",
    "        if bias:\n",
    "            outputs += tf.get_variable(\"bias_\",\n",
    "                        bias_shape,\n",
    "                        regularizer= tf.contrib.layers.l2_regularizer(scale = 3e-7),\n",
    "                        initializer = tf.zeros_initializer())\n",
    "        if def_shape==3:\n",
    "            outputs = tf.squeeze(outputs,axis=1)\n",
    "        if activation is not None:\n",
    "            return activation(outputs)\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Depthwise Separable Convolutions\"\"\"\n",
    "def depthconv(x, kernel_size, output_filters, scope_name,reuse=None):\n",
    "    with tf.variable_scope(scope_name,reuse=reuse):\n",
    "        shapes = x.shape.as_list()\n",
    "        depthwise_filter = tf.get_variable(\"depthwise_filter\",\n",
    "                                        (kernel_size[0], kernel_size[1], shapes[-1], 1),\n",
    "                                        dtype = tf.float32,\n",
    "                                        regularizer=regularizer,\n",
    "                                        initializer = initializer_relu())\n",
    "        pointwise_filter = tf.get_variable(\"pointwise_filter\",\n",
    "                                        (1,1,shapes[-1],output_filters),\n",
    "                                        dtype = tf.float32,\n",
    "                                        regularizer=regularizer,\n",
    "                                        initializer = initializer_relu())\n",
    "        outputs = tf.nn.separable_conv2d(x,\n",
    "                                        depthwise_filter,\n",
    "                                        pointwise_filter,\n",
    "                                        strides = (1,1,1,1),\n",
    "                                        padding = \"SAME\")\n",
    "        b = tf.get_variable(\"bias\",\n",
    "                outputs.shape[-1],\n",
    "                regularizer=tf.contrib.layers.l2_regularizer(scale = 3e-7),\n",
    "                initializer = tf.zeros_initializer())\n",
    "        outputs += b\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Applies a single head attention layer over V using softmax(QK/d**0.5)V\"\"\"\n",
    "def dot_product_attention(q,k,v,dropout,scope=\"dot_product_attention\"):\n",
    "    \"\"\"\n",
    "    q: a Tensor with shape [batch, heads, length_q, depth_k]\n",
    "    k: a Tensor with shape [batch, heads, length_kv, depth_k]\n",
    "    v: a Tensor with shape [batch, heads, length_kv, depth_v]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        # [batch, num_heads, query_length, memory_length]\n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "        logits = logits/(k.shape.as_list()[-1]**0.5)\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "        # dropping out the attention links for each of the heads\n",
    "        weights = tf.nn.dropout(weights, 1.0 - dropout)\n",
    "        return tf.matmul(weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Applies dot attention in parallel with multiplicity equal to the number of heads specified\"\"\"\n",
    "def multihead_attention(queries, units, num_heads, dropout, memory = None, scope = \"Multi_Head_Attention\",reuse=None):\n",
    "    with tf.variable_scope(scope,reuse=reuse):\n",
    "        # Self attention\n",
    "        if memory is None:\n",
    "            memory = queries\n",
    "        memory = conv(memory, 2 * units, name = \"memory_projection\",reuse=reuse)\n",
    "        query = conv(queries, units, name = \"query_projection\",reuse=reuse)\n",
    "        qshapes = query.shape.as_list()\n",
    "        Q = tf.reshape(query, [qshapes[0],qshapes[1],num_heads,-1])\n",
    "        Q = tf.transpose(Q,[0,2,1,3])\n",
    "\n",
    "        mshapes = memory.shape.as_list()\n",
    "        M = tf.reshape(memory, [qshapes[0],qshapes[1],num_heads*2,-1])\n",
    "        M = tf.transpose(M,[0,2,1,3])\n",
    "        K, V = tf.split(M,2,axis=1)\n",
    "\n",
    "        x = dot_product_attention(Q,K,V,dropout)\n",
    "        \n",
    "        shapes = x.shape.as_list()\n",
    "        return tf.reshape(tf.transpose(x,[0,2,1,3]),[shapes[0],shapes[2],-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Applies num_conv_layers convolutions after layer normalization in num_blocks, then a multihead attention\n",
    "    and finally, a feed forward layer\"\"\"\n",
    "def encoderblock(x, kernel_size, output_filters, num_conv_layers, scope_name, drop, num_blocks=1, reuse=None):\n",
    "    with tf.variable_scope(scope_name, reuse=reuse):\n",
    "        x = tf.expand_dims(x,axis=1)\n",
    "        x = conv(x,output_filters,name=\"conv0\",reuse=reuse)\n",
    "        for _ in range(num_blocks):\n",
    "            with tf.variable_scope(\"Block\"+str(_)):\n",
    "                for i in range(num_conv_layers):\n",
    "                    if (i) % 2 == 0:\n",
    "                        x = tf.nn.dropout(x, 1-drop)\n",
    "                    if len(x.shape.as_list())==3:\n",
    "                        x=tf.expand_dims(x,axis=1)\n",
    "                    y = tf.contrib.layers.layer_norm(x,scope=\"layer_norm1_%d\"%i,reuse=reuse)\n",
    "                    y = depthconv(y, kernel_size, output_filters, 'dconv'+str(i),reuse=reuse)\n",
    "                    x = layer_dropout(y,x,drop*float(_*(num_conv_layers+2)+i+1)/float((num_conv_layers + 2) * num_blocks))\n",
    "                x_res1 = tf.squeeze(x, axis=1)\n",
    "                x = tf.contrib.layers.layer_norm(x_res1,scope=\"layer_norm2\",reuse=reuse)\n",
    "                x = tf.nn.dropout(x,1-drop)\n",
    "                x = multihead_attention(x,output_filters,8,reuse=reuse,dropout=drop)\n",
    "                x_res2 = layer_dropout(x,x_res1,drop*float(_*(num_conv_layers+2)+num_conv_layers+1)/float((num_conv_layers + 2) * num_blocks))\n",
    "\n",
    "                x = tf.contrib.layers.layer_norm(x_res2,scope=\"layer_norm3\",reuse=reuse)\n",
    "                x = tf.nn.dropout(x,1-drop)\n",
    "                x = conv(x,output_filters,True,activation=tf.nn.relu,name=\"FFN1\",reuse=reuse)\n",
    "                x = conv(x,output_filters,True,activation=None,name=\"FFN2\",reuse=reuse)\n",
    "                x = layer_dropout(x,x_res2,drop*float(_*(num_conv_layers+2)+num_conv_layers+2)/float((num_conv_layers + 2) * num_blocks))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"An n-layer Highway network with each layer consisting of 2 convolutions\"\"\"\n",
    "def highway(x, drop, size = None, activation = None, num_layers = 2, scope = \"highway\",reuse=None):\n",
    "    with tf.variable_scope(scope,reuse=reuse):\n",
    "        if size is None:\n",
    "            size = x.shape.as_list()[-1]\n",
    "        else:\n",
    "            x = conv(x, size, name = \"input_projection\",reuse=reuse)\n",
    "        for i in range(num_layers):\n",
    "            T = conv(x, size, bias = True, activation = tf.sigmoid,\n",
    "                     name = \"gate_%d\"%i,reuse=reuse)\n",
    "            H = conv(x, size, bias = True, activation = activation,\n",
    "                     name = \"activation_%d\"%i,reuse=reuse)\n",
    "            H = tf.nn.dropout(H, 1-drop)\n",
    "            x = H * T + x * (1.0 - T)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list ranges\n",
    "        over the devices. The inner list ranges over the different variables.\n",
    "    Returns:\n",
    "            List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "            across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = [g for g, _ in grad_and_vars]\n",
    "        grad = tf.reduce_mean(tf.stack(grads), 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Starting Learning Rate\"\"\"\n",
    "starter_learning_rate = tf.placeholder(tf.float32,shape=[])\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "# ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           4000, 1, staircase=True)\n",
    "# Passing global_step to minimize() will increment it at each step.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Placeholders\"\"\"\n",
    "tower_grads = []\n",
    "losses = []\n",
    "drop = tf.placeholder_with_default(0.0, shape=())\n",
    "question_bool = tf.placeholder(tf.float32,[batch_size,q_words],name=\"question_bool\")\n",
    "question_word = tf.placeholder(tf.float32,[batch_size,q_words,word_embedding_dim],name=\"question_word\")\n",
    "question_char = tf.placeholder(tf.int32,[batch_size,q_words,None],name=\"question_char\")\n",
    "context_bool = tf.placeholder(tf.float32,[batch_size,c_words],name=\"context_bool\")\n",
    "context_word = tf.placeholder(tf.float32,[batch_size,c_words,word_embedding_dim],name=\"context_word\")\n",
    "context_char = tf.placeholder(tf.int32,[batch_size,c_words,None],name=\"context_char\")\n",
    "start_ans = tf.placeholder(tf.int32,[batch_size],name=\"start_ans\")\n",
    "end_ans = tf.placeholder(tf.int32,[batch_size],name=\"end_ans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Model\"):\n",
    "    final_logits1 = []\n",
    "    final_logits2 = []\n",
    "    \"\"\"Random initialization of some required variables\"\"\"\n",
    "    unk = tf.get_variable(\"unk\",(word_embedding_dim),dtype = tf.float32,\n",
    "                                            initializer = initializer())\n",
    "    char_emb = tf.get_variable(\"char_emb\",dtype = tf.float32,\n",
    "                                            initializer =tf.constant(vocab_emb, dtype=tf.float32))\n",
    "    zero_word = tf.constant(np.zeros((300)),dtype=tf.float32)\n",
    "    with tf.variable_scope(tf.get_variable_scope()) as outer_scope:\n",
    "        unk_tiled = tf.expand_dims(tf.expand_dims(unk,axis=0),axis=0)\n",
    "        zero_tiled = tf.expand_dims(tf.expand_dims(zero_word,axis=0),axis=0)\n",
    "\n",
    "        question_bool1 = tf.expand_dims(question_bool,axis=-1)\n",
    "        context_bool1 = tf.expand_dims(context_bool,axis=-1)\n",
    "#         print question_bool1\n",
    "        question_word = (question_bool1*(2-question_bool1)*question_word)+(0.5*(1-question_bool1)*(2-question_bool1)*unk_tiled)+(0.5*question_bool1*(question_bool1-1)*zero_tiled)\n",
    "        print question_word\n",
    "        question_word = tf.nn.dropout(question_word,1-drop)\n",
    "\n",
    "        context_word = (context_bool1*(2-context_bool1)*context_word)+(0.5*(1-context_bool1)*(2-context_bool1)*unk_tiled)+(0.5*context_bool1*(context_bool1-1)*zero_tiled)\n",
    "        context_word = tf.nn.dropout(context_word,1-drop)\n",
    "\n",
    "\n",
    "#         print char_emb\n",
    "        question_char_new = tf.nn.embedding_lookup(char_emb,question_char)\n",
    "        question_char_new = tf.nn.dropout(question_char_new,1-(drop*0.5))\n",
    "\n",
    "#         print question_char_new\n",
    "        context_char_new = tf.nn.embedding_lookup(char_emb,context_char)\n",
    "        context_char_new = tf.nn.dropout(context_char_new,1-(drop*0.5))\n",
    "\n",
    "#         print context_char_new\n",
    "        with tf.variable_scope(\"Input_Embedding_Layer\"):\n",
    "            \"\"\"Convolution over char embedding, max pooling and highway network\"\"\"\n",
    "            question_char1 = conv(question_char_new,hidden_layer_size,kernel_size=5,name=\"q_char_conv0\",activation=tf.nn.relu)\n",
    "            question_char1 = tf.reduce_max(question_char1,axis=2)\n",
    "            question_emb = tf.concat([question_word,question_char1],axis=-1)\n",
    "            question_emb = highway(question_emb, drop, scope=\"highway\")\n",
    "#                     print question_emb\n",
    "\n",
    "            \"\"\"Convolution over char embedding, max pooling and highway network\"\"\"\n",
    "            context_char1 = conv(context_char_new,hidden_layer_size,kernel_size=5,name=\"c_char_conv0\",activation=tf.nn.relu)\n",
    "            context_char1 = tf.reduce_max(context_char1,axis=2)\n",
    "            context_emb = tf.concat([context_word,context_char1],axis=-1)\n",
    "            context_emb = highway(context_emb, drop, scope=\"highway\",reuse=True)\n",
    "#             print context_emb\n",
    "\n",
    "\n",
    "        with tf.variable_scope(\"Embedding_encoding_layer\"):\n",
    "            question = encoderblock(question_emb, (1,7), hidden_layer_size,scope_name=\"encoder_block\",num_conv_layers=4,drop=drop)\n",
    "#             print question\n",
    "\n",
    "            context = encoderblock(context_emb, (1,7), hidden_layer_size,scope_name=\"encoder_block\",num_conv_layers=4,reuse=True,drop=drop)\n",
    "#             print context\n",
    "\n",
    "        with tf.variable_scope(\"Context-query_attention\"):\n",
    "            \"\"\"Similarity matrix computation\"\"\"\n",
    "            question = tf.nn.dropout(question,1-drop)\n",
    "            q = tf.tile(tf.expand_dims(question,axis=1),[1,c_words,1,1])\n",
    "            context = tf.nn.dropout(context,1-drop)\n",
    "            c = tf.tile(tf.expand_dims(context,axis=2),[1,1,q_words,1])\n",
    "            s = conv(tf.concat([q,c,tf.multiply(q,c)],axis=-1),1,name=\"similarity_matrix\")\n",
    "            s = tf.squeeze(s,axis=-1)\n",
    "            print s\n",
    "            s_ = tf.nn.softmax(s,axis=-1)\n",
    "            a = tf.matmul(s_,question)\n",
    "#             print a\n",
    "            b = tf.matmul(s_, tf.matmul(tf.transpose(tf.nn.softmax(s,axis=1),[0,2,1]),context))\n",
    "#             print b\n",
    "\n",
    "        with tf.variable_scope(\"Model_encoder_layer\"):\n",
    "            \"\"\"3 encoder blocks with shared parameters\"\"\"\n",
    "            enc_input = tf.concat([context,a,context*a,context*b],axis=-1)\n",
    "#             print enc_input\n",
    "            enc_input = conv(enc_input,hidden_layer_size,name=\"conv0\")\n",
    "            enc_input = encoderblock(enc_input, kernel_size=(1,7), output_filters=hidden_layer_size,scope_name=\"encoder_layer\", num_conv_layers=2, num_blocks=5,drop=drop)\n",
    "#             print enc_input\n",
    "            output_list = [enc_input]\n",
    "            for i in range(model_encoder_layers-1):\n",
    "                temp = encoderblock(output_list[i], kernel_size=(1,7), output_filters=hidden_layer_size,scope_name=\"encoder_layer\", num_conv_layers=2, num_blocks=5,drop=drop,reuse=True)\n",
    "                print temp\n",
    "                output_list.append(temp)\n",
    "\n",
    "        with tf.variable_scope(\"Output_Layer\"):\n",
    "            \"\"\"Softmax followed by loss function calculation\"\"\"\n",
    "            start_logits = tf.squeeze(conv(tf.concat([output_list[0], output_list[1]],axis = -1),1, bias = False, name = \"start_pointer\"),-1)\n",
    "            end_logits = tf.squeeze(conv(tf.concat([output_list[0], output_list[2]],axis = -1),1, bias = False, name = \"end_pointer\"), -1)\n",
    "\n",
    "            logits1 = tf.nn.softmax(start_logits)\n",
    "            logits2 = tf.nn.softmax(end_logits)\n",
    "            print logits1\n",
    "            final_logits1.append(logits1)\n",
    "            final_logits2.append(logits2)\n",
    "            start = tf.one_hot(start_ans,c_words)\n",
    "            end = tf.one_hot(end_ans,c_words)\n",
    "            print start\n",
    "            loss1 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=start_logits,labels=start)\n",
    "            loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=end_logits,labels=end)\n",
    "            loss = loss1+loss2\n",
    "            print loss\n",
    "\n",
    "            grads = optimizer.compute_gradients(loss)\n",
    "            tower_grads.append(grads)\n",
    "\n",
    "            losses.append(loss)                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Apply regularization to loss\"\"\"\n",
    "gradients = average_gradients(tower_grads)\n",
    "avg_loss = tf.reduce_mean(tf.stack(losses))\n",
    "apply_gradient_op = optimizer.apply_gradients(gradients, global_step)\n",
    "logits1_f = tf.concat(final_logits1, axis=0)\n",
    "logits2_f = tf.concat(final_logits2, axis=0)\n",
    "# params = tf.trainable_variables()\n",
    "l2_loss = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(scale = 3e-7),tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "avg_loss+=l2_loss\n",
    "# with tf.control_dependencies([apply_gradient_op]):\n",
    "#     # Create the shadow variables, and add ops to maintain moving averages\n",
    "#     # of var0 and var1. This also creates an op that will update the moving\n",
    "#     # averages after each training step.  This is what we will use in place\n",
    "#     # of the usual training op.\n",
    "#     training_op = ema.apply(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Random variables initialization\"\"\"\n",
    "init_op = tf.global_variables_initializer()\n",
    "config=tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.restore(sess, \"saved_model2/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"F1 score calculation from predicted answer and actual answer in list format\"\"\"\n",
    "def compute_f1(ans_predicted, ans_actual):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    for gold_toks, pred_toks in zip(ans_actual, ans_predicted):\n",
    "        common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "        num_same = sum(common.values())\n",
    "        tp+=num_same\n",
    "        fp+=(len(pred_toks)-num_same)\n",
    "        tn+=(len(gold_toks)-num_same)\n",
    "    return tp,fp,tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_demo.pkl\") as f:\n",
    "    test_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Converts a mini batch of training dataset, applies required padding and returns a feed dict\"\"\"\n",
    "def train(x):\n",
    "    feed_dict={}\n",
    "    q_w = []\n",
    "    q_c = []\n",
    "    c_w = []\n",
    "    c_c = []\n",
    "    a_s = []\n",
    "    a_e = []\n",
    "    q_b = []\n",
    "    c_b = []\n",
    "    for i in x:\n",
    "        zero_w = np.zeros((q_words-len(i['questionword']),word_embedding_dim))\n",
    "        q_w.append(np.concatenate((i['questionword'],zero_w),axis=0))\n",
    "        zero_w = np.zeros((c_words-len(i['contextword']),word_embedding_dim))\n",
    "        c_w.append(np.concatenate((i['contextword'],zero_w),axis=0)) \n",
    "        q_b.append(np.pad(i['questionbool'],[0,q_words-len(i['questionword'])],'constant', constant_values=(2)))\n",
    "        c_b.append(np.pad(i['contextbool'],[0,c_words-len(i['contextword'])],'constant', constant_values=(2)))\n",
    "        zero_c = (no_of_chars-1)*np.ones((q_words-len(i['questionchar']),max_word_len))\n",
    "        q_c.append(np.concatenate((i['questionchar'],zero_c),axis=0))\n",
    "        zero_c = (no_of_chars-1)*np.ones((c_words-len(i['contextchar']),max_word_len))\n",
    "        c_c.append(np.concatenate((i['contextchar'],zero_c),axis=0))\n",
    "        a_s.append(i['answer_start'])\n",
    "        a_e.append(i['answer_end'])\n",
    "    feed_dict = {\n",
    "        question_word:np.array(q_w),\n",
    "        context_word:np.array(c_w),\n",
    "        question_char:np.array(q_c),\n",
    "        context_char:np.array(c_c),\n",
    "        start_ans:np.array(a_s),\n",
    "        end_ans:np.array(a_e),\n",
    "        question_bool:np.array(q_b),\n",
    "        context_bool:np.array(c_b)\n",
    "    }\n",
    "\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate results\"\"\"\n",
    "def test(x_test,max_f1,testing=True):\n",
    "        tp=0\n",
    "        fp=0\n",
    "        tn=0\n",
    "        no_of_test_batches=len(x_test)/batch_size\n",
    "        t_ptr=0\n",
    "        for tbatch in range(no_of_test_batches):\n",
    "            test_feed = train(x_test[t_ptr:t_ptr+batch_size])\n",
    "            t_ptr+=batch_size\n",
    "            np_ans_st = sess.run(logits1_f,test_feed)\n",
    "            np_ans_e = sess.run(logits2_f,test_feed)\n",
    "            ans_st = np.argmax(np_ans_st,axis=-1)\n",
    "            ans_e = np.argmax(np_ans_e,axis=-1)\n",
    "            ans_pred=[range(x,y) for x,y in zip(ans_st,ans_e)]\n",
    "            ans_ac=[range(x,y) for x,y in zip(test_feed[start_ans],test_feed[end_ans])]\n",
    "            x,y,z=compute_f1(ans_pred, ans_ac)\n",
    "            tp+=x\n",
    "            fp+=y\n",
    "            tn+=z\n",
    "#             if tbatch==0:\n",
    "#                 print np_ans_st[0]\n",
    "            if tbatch%10==0:\n",
    "                print(\"Testing %d\" % tbatch),\n",
    "                if testing:\n",
    "                    print \"On testing data\"\n",
    "                else:\n",
    "                    print \"On training data\"\n",
    "                print \"Start: \",np.argmax(np_ans_st,axis=-1)[0],\n",
    "                print \"End: \",np.argmax(np_ans_e,axis=-1)[0]\n",
    "                print \"Start Actual: \",test_feed[start_ans][0],\n",
    "                print \"End Actual: \",test_feed[end_ans][0]\n",
    "#             if tbatch==50:\n",
    "#                 print \"Global Step: \",sess.run(global_step)\n",
    "#                 break\n",
    "        print tp,fp,tn\n",
    "        try:\n",
    "            precision = 1.0 * tp / (tp+fp)\n",
    "            recall = 1.0 * tp / (tp+tn)\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "            print(\"F1 Score:\")+str(f1)\n",
    "        except:\n",
    "            print(\"F1 Score:N/A\")\n",
    "            if testing:\n",
    "                save_path = saver.save(sess, \"./saved_model2/model.ckpt\")\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "            return max_f1\n",
    "        if f1>max_f1 and testing:\n",
    "            save_path = saver.save(sess, \"./saved_model2/model.ckpt\")\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "            max_f1 = max(f1,max_f1)\n",
    "        else:\n",
    "            print \"Not Saved\"\n",
    "        return max(max_f1,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = test(test_data,0,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_func(x):\n",
    "    feed_dict={}\n",
    "    q_w = []\n",
    "    q_c = []\n",
    "    c_w = []\n",
    "    c_c = []\n",
    "    a_s = []\n",
    "    a_e = []\n",
    "    q_b = []\n",
    "    c_b = []\n",
    "    for i in x:\n",
    "        zero_w = np.zeros((q_words-len(i['questionword']),word_embedding_dim))\n",
    "        q_w.append(np.concatenate((i['questionword'],zero_w),axis=0))\n",
    "        zero_w = np.zeros((c_words-len(i['contextword']),word_embedding_dim))\n",
    "        c_w.append(np.concatenate((i['contextword'],zero_w),axis=0)) \n",
    "        q_b.append(np.pad(i['questionbool'],[0,q_words-len(i['questionword'])],'constant', constant_values=(2)))\n",
    "        c_b.append(np.pad(i['contextbool'],[0,c_words-len(i['contextword'])],'constant', constant_values=(2)))\n",
    "        zero_c = (no_of_chars-1)*np.ones((q_words-len(i['questionchar']),max_word_len))\n",
    "        q_c.append(np.concatenate((i['questionchar'],zero_c),axis=0))\n",
    "        zero_c = (no_of_chars-1)*np.ones((c_words-len(i['contextchar']),max_word_len))\n",
    "        c_c.append(np.concatenate((i['contextchar'],zero_c),axis=0))\n",
    "    feed_dict = {\n",
    "        question_word:np.array(q_w),\n",
    "        context_word:np.array(c_w),\n",
    "        question_char:np.array(q_c),\n",
    "        context_char:np.array(c_c),\n",
    "        question_bool:np.array(q_b),\n",
    "        context_bool:np.array(c_b)\n",
    "    }\n",
    "\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Returns the answer to the question given context\"\"\"\n",
    "def get_answer(context,question):\n",
    "    c_word,c_char,c_bool,c=preprocess(context)\n",
    "    q_word,q_char,q_bool,q=preprocess(question)\n",
    "    data = {'contextword':c_word,'contextchar':c_char,'contextbool':c_bool,'questionword':q_word,'questionchar':q_char,'questionbool':q_bool}\n",
    "    example_feed = [data for _ in range(batch_size)]\n",
    "    temp1 = np.argmax(sess.run(logits1,testing_func(example_feed))[0])\n",
    "    temp2 = np.argmax(sess.run(logits2,testing_func(example_feed))[0])\n",
    "    try:\n",
    "        return context[c[temp1]:c[temp2]]\n",
    "    except:\n",
    "        return context[c[temp1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=u\"\"\"Michael John Clarke (born 2 April 1981) is a former Australian international cricketer. He led Australia to their 5th Cricket World Cup triumph, when his team were victorious in the final of the ICC Cricket World Cup 2015 at the MCG. He is the first cap for Australia in Twenty20 Internationals. His ODI shirt number of 23 was passed on to him by Shane Warne after his international retirement. Nicknamed \"Pup\",[3] he is a right-handed middle-order batsman, an occasional left-arm orthodox spin bowler and also a slip catcher. He represented New South Wales at a domestic level. In January 2011, Clarke stood down as captain of the Australian Twenty20 cricket team to concentrate on his Test and ODI performance. After announcing he would retire from One Day cricket after the end of the 2015 Cricket World Cup, Clarke starred in the final against New Zealand top scoring with a score of 74 off 72 balls, as Australia won their fifth World Cup title. He was bowled when nine runs were required to win and received a standing ovation from the 93,013 strong MCG crowd after his dismissal. On 8 August 2015, Clarke announced that he would retire from all forms of cricket after the final Test of the 2015 Ashes series following a difficult series in terms of both his and the team's performance. Australia suffered a crushing defeat of an innings and 78 runs thus losing the Ashes. This was Clarke's fourth successive Ashes loss in England overall and his second as captain.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=u\"\"\"When was Michael Clarke born?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_answer(context,question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
