{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dim=300\n",
    "char_embedding_dim=200\n",
    "q_words=50\n",
    "c_words=400\n",
    "embedding_dim = word_embedding_dim+char_embedding_dim\n",
    "batch_size=2\n",
    "model_encoder_layers=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def loadGloveModel(gloveFile):\n",
    "#     print \"Loading Glove Model\"\n",
    "#     f = open(gloveFile,'r')\n",
    "#     model = {}\n",
    "#     for line in f:\n",
    "#         splitLine = line.split()\n",
    "#         word = splitLine[0]\n",
    "#         embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "#         model[word] = embedding\n",
    "#     print \"Done.\",len(model),\" words loaded!\"\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = loadGloveModel(\"glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model[\"Hello\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(inputs, output_size, bias = None, activation = None, kernel_size = 1, name = \"conv\"):\n",
    "    with tf.variable_scope(name):\n",
    "        def_shape=4\n",
    "        if len(inputs.shape)==3:\n",
    "            inputs = tf.expand_dims(inputs,axis=1)\n",
    "            def_shape=3\n",
    "        shapes = inputs.shape.as_list()\n",
    "        filter_shape = [1,kernel_size,shapes[-1],output_size]\n",
    "        bias_shape = [1,1,1,output_size]\n",
    "        strides = [1,1,1,1]\n",
    "        kernel_ = tf.get_variable(\"kernel_\",\n",
    "                        filter_shape,\n",
    "                        dtype = tf.float32,\n",
    "                        regularizer= tf.contrib.layers.l2_regularizer(scale = 3e-7),\n",
    "                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "        outputs = tf.nn.conv2d(inputs, kernel_, strides, \"VALID\")\n",
    "        if bias:\n",
    "            outputs += tf.get_variable(\"bias_\",\n",
    "                        bias_shape,\n",
    "                        regularizer= tf.contrib.layers.l2_regularizer(scale = 3e-7),\n",
    "                        initializer = tf.zeros_initializer())\n",
    "        if def_shape==3:\n",
    "            outputs = tf.squeeze(outputs,axis=1)\n",
    "        if activation is not None:\n",
    "            return activation(outputs)\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depthconv(x, kernel_size, output_filters, scope_name):\n",
    "    with tf.variable_scope(scope_name):\n",
    "        shapes = x.shape.as_list()\n",
    "        depthwise_filter = tf.get_variable(\"depthwise_filter\",\n",
    "                                        (kernel_size[0], kernel_size[1], shapes[-1], 1),\n",
    "                                        dtype = tf.float32,\n",
    "                                        regularizer= tf.contrib.layers.l2_regularizer(scale = 3e-7),\n",
    "                                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "        pointwise_filter = tf.get_variable(\"pointwise_filter\",\n",
    "                                        (1,1,shapes[-1],output_filters),\n",
    "                                        dtype = tf.float32,\n",
    "                                        regularizer=tf.contrib.layers.l2_regularizer(scale = 3e-7),\n",
    "                                        initializer = tf.contrib.layers.xavier_initializer())\n",
    "        outputs = tf.nn.separable_conv2d(x,\n",
    "                                        depthwise_filter,\n",
    "                                        pointwise_filter,\n",
    "                                        strides = (1,1,1,1),\n",
    "                                        padding = \"SAME\")\n",
    "        b = tf.get_variable(\"bias\",\n",
    "                outputs.shape[-1],\n",
    "                regularizer=tf.contrib.layers.l2_regularizer(scale = 3e-7),\n",
    "                initializer = tf.zeros_initializer())\n",
    "        outputs += b\n",
    "        outputs = tf.nn.relu(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(q,k,v,scope=\"dot_product_attention\"):\n",
    "    \"\"\"\n",
    "    q: a Tensor with shape [batch, heads, length_q, depth_k]\n",
    "    k: a Tensor with shape [batch, heads, length_kv, depth_k]\n",
    "    v: a Tensor with shape [batch, heads, length_kv, depth_v]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        # [batch, num_heads, query_length, memory_length]\n",
    "        logits = tf.matmul(q, k, transpose_b=True)\n",
    "        logits = logits/(k.shape.as_list()[-1]**0.5)\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "        # dropping out the attention links for each of the heads\n",
    "        return tf.matmul(weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(queries, units, num_heads, memory = None, scope = \"Multi_Head_Attention\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        # Self attention\n",
    "        if memory is None:\n",
    "            memory = queries\n",
    "        memory = conv(memory, 2 * units, name = \"memory_projection\")\n",
    "        query = conv(queries, units, name = \"query_projection\")\n",
    "        qshapes = query.shape.as_list()\n",
    "        Q = tf.reshape(query, [qshapes[0],qshapes[1],num_heads,-1])\n",
    "        Q = tf.transpose(Q,[0,2,1,3])\n",
    "\n",
    "        mshapes = memory.shape.as_list()\n",
    "        M = tf.reshape(memory, [qshapes[0],qshapes[1],num_heads*2,-1])\n",
    "        M = tf.transpose(M,[0,2,1,3])\n",
    "        K, V = tf.split(M,2,axis=1)\n",
    "\n",
    "        x = dot_product_attention(Q,K,V)\n",
    "        \n",
    "        shapes = x.shape.as_list()\n",
    "        return tf.reshape(tf.transpose(x,[0,2,1,3]),[shapes[0],shapes[2],-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoderblock(x, kernel_size, output_filters, num_conv_layers, scope_name, num_blocks=1, reuse=False):\n",
    "    with tf.variable_scope(scope_name, reuse=reuse):\n",
    "        x = tf.expand_dims(x,axis=1)\n",
    "        x = conv(x,output_filters,name=\"conv0\")\n",
    "        for _ in range(num_blocks):\n",
    "            with tf.variable_scope(\"Block\"+str(_)):\n",
    "                for i in range(num_conv_layers):\n",
    "                    if len(x.shape.as_list())==3:\n",
    "                        x=tf.expand_dims(x,axis=1)\n",
    "                    y = tf.contrib.layers.layer_norm(x)\n",
    "                    y = depthconv(y, kernel_size, output_filters, 'dconv'+str(i))\n",
    "\n",
    "                    x = x+y\n",
    "                x = tf.squeeze(x, axis=1)\n",
    "                x = tf.contrib.layers.layer_norm(x)\n",
    "                x = multihead_attention(x,output_filters,8)\n",
    "                x = tf.contrib.layers.layer_norm(x)\n",
    "                x = conv(x,output_filters,True,activation=tf.nn.relu,name=\"FFN\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q=tf.placeholder(tf.float32,[batch_size,q_words,embedding_dim])\n",
    "# print encoderblock(q,(7,1),128,4,\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highway(x, size = None, activation = None, num_layers = 2, scope = \"highway\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        if size is None:\n",
    "            size = x.shape.as_list()[-1]\n",
    "        else:\n",
    "            x = conv(x, size, name = \"input_projection\")\n",
    "        for i in range(num_layers):\n",
    "            T = conv(x, size, bias = True, activation = tf.sigmoid,\n",
    "                     name = \"gate_%d\"%i)\n",
    "            H = conv(x, size, bias = True, activation = activation,\n",
    "                     name = \"activation_%d\"%i)\n",
    "            x = H * T + x * (1.0 - T)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_word = tf.placeholder(tf.float32,[batch_size,q_words,word_embedding_dim])\n",
    "question_char = tf.placeholder(tf.float32,[batch_size,q_words,None, char_embedding_dim])\n",
    "context_word = tf.placeholder(tf.float32,[batch_size,c_words,word_embedding_dim])\n",
    "context_char = tf.placeholder(tf.float32,[batch_size,c_words,None, char_embedding_dim])\n",
    "start_ans = tf.placeholder(tf.int32,[batch_size])\n",
    "end_ans = tf.placeholder(tf.int32,[batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Input_Embedding_Layer\"):\n",
    "    question_char1 = conv(question_char,char_embedding_dim,kernel_size=5,name=\"q_char_conv0\")\n",
    "    question_char1 = tf.reduce_max(question_char1,axis=2)\n",
    "    question_emb = tf.concat([question_word,question_char1],axis=-1)\n",
    "    question_emb = highway(question_emb, scope=\"q_highway\")\n",
    "    print question_emb\n",
    "    \n",
    "    context_char1 = conv(context_char,char_embedding_dim,kernel_size=5,name=\"c_char_conv0\")\n",
    "    context_char1 = tf.reduce_max(context_char1,axis=2)\n",
    "    context_emb = tf.concat([context_word,context_char1],axis=-1)\n",
    "    context_emb = highway(context_emb, scope=\"c_highway\")\n",
    "    print context_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Embedding_encoding_layer\"):\n",
    "    question = encoderblock(question_emb, (1,7), 128, 4, \"q_encoder_block\")\n",
    "    print question\n",
    "\n",
    "    context = encoderblock(context_emb, (1,7), 128, 4, \"c_encoder_block\")\n",
    "    print context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Context-query_attention\"):\n",
    "    q = tf.tile(tf.expand_dims(question,axis=1),[1,c_words,1,1])\n",
    "    c = tf.tile(tf.expand_dims(context,axis=2),[1,1,q_words,1])\n",
    "    s = conv(tf.concat([q,c,tf.multiply(q,c)],axis=-1),1,name=\"similarity_matrix\")\n",
    "    s = tf.squeeze(s,axis=-1)\n",
    "    print s\n",
    "    s_ = tf.nn.softmax(s,axis=-1)\n",
    "    a = tf.matmul(s_,question)\n",
    "    print a\n",
    "    b = tf.matmul(s_, tf.matmul(tf.transpose(tf.nn.softmax(s,axis=1),[0,2,1]),context))\n",
    "    print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Model_encoder_layer\"):\n",
    "    enc_input = tf.concat([context,a,context*a,context*b],axis=-1)\n",
    "    print enc_input\n",
    "    enc_input = conv(enc_input,128,name=\"conv0\")\n",
    "    enc_input = encoderblock(enc_input, (1,7), 128, 2, \"encoder_layer\", 7)\n",
    "    print enc_input\n",
    "    output_list = [enc_input]\n",
    "    for i in range(model_encoder_layers-1):\n",
    "        temp = encoderblock(output_list[i], (1,7), 128, 2, \"encoder_layer\", 7, True)\n",
    "        print temp\n",
    "        output_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Output_Layer\"):\n",
    "    start_logits = tf.squeeze(conv(tf.concat([output_list[0], output_list[1]],axis = -1),1, bias = False, name = \"start_pointer\"),-1)\n",
    "    end_logits = tf.squeeze(conv(tf.concat([output_list[0], output_list[2]],axis = -1),1, bias = False, name = \"end_pointer\"), -1)\n",
    "    \n",
    "    logits1 = tf.nn.softmax(start_logits)\n",
    "    logits2 = tf.nn.softmax(end_logits)\n",
    "    print logits1\n",
    "    \n",
    "    start = tf.one_hot(start_ans,c_words)\n",
    "    end = tf.one_hot(end_ans,c_words)\n",
    "    print start\n",
    "    start = tf.log(tf.multiply(start,logits1))\n",
    "    end = tf.log(tf.multiply(end,logits2))\n",
    "    losses = -(tf.reduce_mean(start)+tf.reduce_mean(end))\n",
    "    print losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "q1 = np.random.randn(batch_size,q_words,word_embedding_dim)\n",
    "q2 = np.random.randn(batch_size,q_words,20, char_embedding_dim)\n",
    "c1 = np.random.randn(batch_size,c_words,word_embedding_dim)\n",
    "c2 = np.random.randn(batch_size,c_words,20, char_embedding_dim)\n",
    "s1 = np.array([1]*batch_size)\n",
    "e1 = np.array([2]*batch_size)\n",
    "print sess.run(losses,{question_word:q1, question_char:q2,context_word:c1,context_char:c2,start_ans:s1,end_ans:e1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
